wandb_version: 1

algorithm:
  desc: null
  value:
    clip_param: 0.2
    desired_kl: 0.01
    entropy_coef: 0.01
    gamma: 0.99
    lam: 0.95
    learning_rate: 1.0e-05
    max_grad_norm: 1.0
    num_learning_epochs: 5
    num_mini_batches: 4
    schedule: fixed
    use_clipped_value_loss: true
    value_loss_coef: 1.0
init_member_classes:
  desc: null
  value: {}
policy:
  desc: null
  value:
    activation: elu
    actor_hidden_dims:
    - 512
    - 256
    - 128
    critic_hidden_dims:
    - 512
    - 256
    - 128
    init_noise_std: 1.0
runner:
  desc: null
  value:
    algorithm_class_name: PPO
    checkpoint: -2
    experiment_name: rough_iust
    load_run: -2
    max_iterations: 1500
    num_steps_per_env: 24
    policy_class_name: ActorCritic
    resume: false
    resume_path: null
    run_name: ''
    save_interval: 50
runner_class_name:
  desc: null
  value: OnPolicyRunner
seed:
  desc: null
  value: 1
_wandb:
  desc: null
  value:
    python_version: 3.8.10
    cli_version: 0.17.0
    framework: torch
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1716031382
    t:
      1:
      - 1
      - 55
      3:
      - 13
      - 16
      - 23
      - 35
      4: 3.8.10
      5: 0.17.0
      8:
      - 5
      13: linux-x86_64
